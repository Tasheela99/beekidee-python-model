import importlib
import types
import sys
import pytest
import os
import webbrowser
from datetime import datetime, timedelta
from typing import Any, Dict, List

# Console colors
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

# Global variable to track test log file
test_log_file = None
test_results = {}

def print_test_header(test_name: str):
    """Print a beautiful test header"""
    print("\n" + "=" * 80)
    print(f"{Colors.HEADER}{Colors.BOLD}üß™ RUNNING TEST: {test_name}{Colors.ENDC}")
    print("=" * 80)

def print_test_step(step: str):
    """Print a test step with formatting"""
    print(f"{Colors.OKCYAN}üìù {step}{Colors.ENDC}")

def print_test_result(result_type: str, message: str):
    """Print test results with appropriate colors"""
    if result_type == "PASS":
        print(f"{Colors.OKGREEN}‚úÖ {message}{Colors.ENDC}")
    elif result_type == "FAIL":
        print(f"{Colors.FAIL}‚ùå {message}{Colors.ENDC}")
    elif result_type == "WARNING":
        print(f"{Colors.WARNING}‚ö†Ô∏è  {message}{Colors.ENDC}")
    elif result_type == "INFO":
        print(f"{Colors.OKBLUE}‚ÑπÔ∏è  {message}{Colors.ENDC}")

def print_response_data(label: str, data: Any):
    """Print response data with formatting"""
    print(f"{Colors.OKCYAN}üìä {label}:{Colors.ENDC}")
    print(f"   {data}")

def log_test_result(test_name: str, message: str):
    global test_log_file
    if test_log_file is None:
        # Create test-results directory if it doesn't exist
        test_results_dir = "test-results"
        os.makedirs(test_results_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        test_log_file = open(f"{test_results_dir}/test_results_{timestamp}.txt", "w")
        test_log_file.write(f"Test Results - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        test_log_file.write("=" * 50 + "\n\n")
    
    test_log_file.write(f"[{test_name}] {message}\n")
    test_log_file.flush()

def log_test_start(test_name: str):
    """Log the start of a new test with spacing"""
    global test_log_file
    if test_log_file:
        test_log_file.write(f"\n{'=' * 60}\n")
        test_log_file.write(f"STARTING TEST: {test_name}\n")
        test_log_file.write(f"{'=' * 60}\n")
        test_log_file.flush()

def log_test_end(test_name: str, status: str):
    """Log the end of a test with spacing"""
    global test_log_file
    if test_log_file:
        test_log_file.write(f"\nTEST COMPLETED: {test_name} - {status}\n")
        test_log_file.write(f"{'-' * 60}\n\n")
        test_log_file.flush()

def record_test_result(test_name: str, status: str, details: str = ""):
    """Record test results for final summary"""
    test_results[test_name] = {"status": status, "details": details}

def generate_html_report():
    """Generate an HTML report with test results and open it in browser"""
    global test_results, test_log_file
    
    # Create test-results directory if it doesn't exist
    test_results_dir = "test-results"
    os.makedirs(test_results_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    html_filename = f"{test_results_dir}/test_report_{timestamp}.html"
    
    passed = sum(1 for result in test_results.values() if result["status"] == "PASS")
    failed = sum(1 for result in test_results.values() if result["status"] == "FAIL")
    total = len(test_results)
    
    html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Results Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }}
        .header {{
            text-align: center;
            margin-bottom: 30px;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 8px;
        }}
        .summary {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        .summary-card {{
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            color: white;
            font-weight: bold;
        }}
        .total {{ background: #3498db; }}
        .passed {{ background: #2ecc71; }}
        .failed {{ background: #e74c3c; }}
        .test-results {{
            margin-top: 30px;
        }}
        .test-item {{
            margin-bottom: 20px;
            padding: 20px;
            border-radius: 8px;
            border-left: 5px solid;
        }}
        .test-pass {{
            background: #d4edda;
            border-color: #28a745;
        }}
        .test-fail {{
            background: #f8d7da;
            border-color: #dc3545;
        }}
        .test-name {{
            font-weight: bold;
            font-size: 1.2em;
            margin-bottom: 10px;
        }}
        .test-status {{
            display: inline-block;
            padding: 5px 10px;
            border-radius: 15px;
            font-size: 0.9em;
            font-weight: bold;
        }}
        .status-pass {{
            background: #28a745;
            color: white;
        }}
        .status-fail {{
            background: #dc3545;
            color: white;
        }}
        .test-details {{
            margin-top: 10px;
            font-style: italic;
            color: #666;
        }}
        .timestamp {{
            text-align: center;
            margin-top: 30px;
            color: #666;
            font-size: 0.9em;
        }}
        .overall-status {{
            text-align: center;
            font-size: 1.5em;
            font-weight: bold;
            margin: 20px 0;
            padding: 15px;
            border-radius: 8px;
        }}
        .all-pass {{
            background: #d4edda;
            color: #155724;
        }}
        .some-fail {{
            background: #f8d7da;
            color: #721c24;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Flask API Test Results</h1>
            <p>Real-time Attention Analysis API Test Suite</p>
        </div>
        
        <div class="summary">
            <div class="summary-card total">
                <h3>{total}</h3>
                <p>Total Tests</p>
            </div>
            <div class="summary-card passed">
                <h3>{passed}</h3>
                <p>Passed</p>
            </div>
            <div class="summary-card failed">
                <h3>{failed}</h3>
                <p>Failed</p>
            </div>
        </div>
        
        <div class="overall-status {'all-pass' if failed == 0 else 'some-fail'}">
            {'üéâ ALL TESTS PASSED! üéâ' if failed == 0 else '‚ö†Ô∏è SOME TESTS FAILED ‚ö†Ô∏è'}
        </div>
        
        <div class="test-results">
            <h2>Detailed Test Results</h2>
    """
    
    for test_name, result in test_results.items():
        status_class = "test-pass" if result["status"] == "PASS" else "test-fail"
        status_badge_class = "status-pass" if result["status"] == "PASS" else "status-fail"
        status_icon = "‚úÖ" if result["status"] == "PASS" else "‚ùå"
        
        html_content += f"""
            <div class="test-item {status_class}">
                <div class="test-name">
                    {status_icon} {test_name}
                    <span class="test-status {status_badge_class}">{result["status"]}</span>
                </div>
                {f'<div class="test-details">{result["details"]}</div>' if result["details"] else ''}
            </div>
        """
    
    html_content += f"""
        </div>
        
        <div class="timestamp">
            <p>Report generated on {datetime.now().strftime('%Y-%m-%d at %H:%M:%S')}</p>
            <p>Test execution completed in {test_results_dir}/</p>
        </div>
    </div>
</body>
</html>
    """
    
    # Write HTML file
    with open(html_filename, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    # Open in browser
    try:
        file_path = os.path.abspath(html_filename)
        webbrowser.open(f'file://{file_path}')
        print(f"\n{Colors.OKGREEN}üìä HTML report generated and opened: {html_filename}{Colors.ENDC}")
    except Exception as e:
        print(f"\n{Colors.WARNING}‚ö†Ô∏è  HTML report generated but couldn't open browser: {html_filename}{Colors.ENDC}")
        print(f"   Error: {e}")
    
    return html_filename

@pytest.fixture(scope="session", autouse=True)
def cleanup_test_log():
    yield
    global test_log_file
    if test_log_file:
        test_log_file.write("\n" + "=" * 50 + "\n")
        test_log_file.write(f"Tests completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        test_log_file.close()
    
    # Print final summary
    print_final_summary()
    
    # Generate and open HTML report
    generate_html_report()

def print_final_summary():
    """Print a beautiful final summary of all tests"""
    print("\n" + "=" * 80)
    print(f"{Colors.HEADER}{Colors.BOLD}üèÅ TEST EXECUTION SUMMARY{Colors.ENDC}")
    print("=" * 80)
    
    passed = sum(1 for result in test_results.values() if result["status"] == "PASS")
    failed = sum(1 for result in test_results.values() if result["status"] == "FAIL")
    total = len(test_results)
    
    print(f"{Colors.BOLD}Total Tests: {total}{Colors.ENDC}")
    print(f"{Colors.OKGREEN}‚úÖ Passed: {passed}{Colors.ENDC}")
    print(f"{Colors.FAIL}‚ùå Failed: {failed}{Colors.ENDC}")
    
    if failed == 0:
        print(f"\n{Colors.OKGREEN}{Colors.BOLD}üéâ ALL TESTS PASSED! üéâ{Colors.ENDC}")
    else:
        print(f"\n{Colors.WARNING}{Colors.BOLD}‚ö†Ô∏è  SOME TESTS FAILED ‚ö†Ô∏è{Colors.ENDC}")
    
    print("\nüìã Detailed Results:")
    for test_name, result in test_results.items():
        status_color = Colors.OKGREEN if result["status"] == "PASS" else Colors.FAIL
        status_icon = "‚úÖ" if result["status"] == "PASS" else "‚ùå"
        print(f"  {status_color}{status_icon} {test_name}: {result['status']}{Colors.ENDC}")
        if result["details"]:
            print(f"    üìù {result['details']}")
    
    print("=" * 80)


class FakeDocSnapshot:
    def __init__(self, data):
        self._data = data

    def to_dict(self):
        return dict(self._data)


class FakeSubCollection:
    def __init__(self, list_ref: List[Dict[str, Any]]):
        self._list = list_ref

    def add(self, doc: Dict[str, Any]):
        self._list.append(dict(doc))
        return ("auto_id", None)


class FakeDocumentRef:
    def __init__(self, db, col_name, doc_id):
        self._db = db
        self._col_name = col_name
        self._doc_id = doc_id
        self._ensure_doc()

    def _ensure_doc(self):
        col = self._db._collections.setdefault(self._col_name, {})
        if self._doc_id not in col:
            col[self._doc_id] = {
                "_meta": {},
                "_sub": {},
            }

    def set(self, data: Dict[str, Any], merge: bool = False):
        col = self._db._collections[self._col_name]
        if merge:
            col[self._doc_id]["_meta"].update(data)
        else:
            col[self._doc_id]["_meta"] = dict(data)

    def collection(self, sub_name: str):
        col = self._db._collections[self._col_name]
        sub = col[self._doc_id]["_sub"].setdefault(sub_name, [])
        return FakeSubCollection(sub)


class FakeQuery:
    def __init__(self, db, col_group_name: str, items: List[Dict[str, Any]]):
        self._db = db
        self._col_group_name = col_group_name
        self._items = items
        self._filters = []
        self._order_by = None
        self._order_dir = "ASC"
        self._limit = None

    def where(self, field, op, value):
        assert op == "==", "Only equality supported in fake"
        self._filters.append((field, value))
        return self

    class _Dir:
        DESCENDING = "DESCENDING"
        ASCENDING = "ASCENDING"

    def order_by(self, field, direction="ASCENDING"):
        self._order_by = field
        self._order_dir = direction
        return self

    def limit(self, n):
        self._limit = n
        return self

    def stream(self):
        result = self._items
        for f, v in self._filters:
            result = [d for d in result if d.get(f) == v]
        if self._order_by:
            reverse = self._order_dir == "DESCENDING"
            result = sorted(
                result,
                key=lambda d: d.get(self._order_by, 0),
                reverse=reverse,
            )
        if self._limit is not None:
            result = result[: self._limit]
        return [FakeDocSnapshot(d) for d in result]


class FakeDB:
    SERVER_TIMESTAMP = object()

    class Query(FakeQuery._Dir):
        pass

    def __init__(self):
        self._collections: Dict[str, Dict[str, Any]] = {}

    def collection(self, name: str):
        class _Col:
            def __init__(self, outer, name):
                self._outer = outer
                self._name = name

            def document(self, doc_id: str):
                return FakeDocumentRef(self._outer, self._name, doc_id)

        return _Col(self, name)

    def collection_group(self, sub_name: str):
        items: List[Dict[str, Any]] = []
        for _, docs in self._collections.items():
            for _, doc_body in docs.items():
                subs = doc_body.get("_sub", {})
                if sub_name in subs:
                    for d in subs[sub_name]:
                        items.append(dict(d))
        return FakeQuery(self, sub_name, items)


@pytest.fixture(scope="session")
def stub_modules():
    fb = types.SimpleNamespace()
    fb._apps = []

    def initialize_app(cred):
        fb._apps.append("inited")

    class _Creds:
        @staticmethod
        def Certificate(path):
            return {"certificate_path": path}

    fb.initialize_app = initialize_app
    fb.credentials = _Creds

    fs = types.SimpleNamespace()
    _fake_db = FakeDB()

    def client():
        return _fake_db

    fs.client = client
    fs.SERVER_TIMESTAMP = _fake_db.SERVER_TIMESTAMP
    fs.Query = _fake_db.Query

    fb.firestore = fs

    ac = types.SimpleNamespace()

    class RealTimeAttentionAnalyzer:
        def __init__(self):
            self.data = {
                "timestamp": [],
                "overall": [],
            }
            self.is_tracking = False

        def run(self):
            pass

    ac.RealTimeAttentionAnalyzer = RealTimeAttentionAnalyzer

    sys.modules["firebase_admin"] = fb
    sys.modules["firebase_admin.credentials"] = _Creds
    sys.modules["firebase_admin.firestore"] = fs
    sys.modules["firestore"] = fs
    sys.modules["attention_count"] = ac

    yield {"fb": fb, "fs": fs, "ac": ac, "db": _fake_db}


@pytest.fixture()
def app_and_client(stub_modules):
    app_module = importlib.import_module("app")
    flask_app = app_module.app
    flask_app.testing = True
    client = flask_app.test_client()
    app_module.db = stub_modules["db"]
    return app_module, client


def test_analyze_requires_uid(app_and_client):
    test_name = "test_analyze_requires_uid"
    log_test_start(test_name)
    print_test_header(test_name)
    
    app_module, client = app_and_client
    
    print_test_step("Sending POST request to /analyze with empty JSON body")
    resp = client.post("/analyze", json={})
    
    print_response_data("Response Status", resp.status_code)
    response_data = resp.get_json()
    print_response_data("Response Data", response_data)
    
    log_test_result(test_name, f"Response status: {resp.status_code}")
    log_test_result(test_name, f"Response data: {response_data}")
    
    try:
        assert resp.status_code == 400
        assert "uid is required" in response_data["error"]
        # Uncomment the line below to force test failure
        # assert resp.status_code == 200 
        print_test_result("PASS", "UID validation working correctly")
        record_test_result(test_name, "PASS", "UID validation enforced properly")
        log_test_result(test_name, "TEST PASSED: UID validation working correctly")
        log_test_end(test_name, "PASS")
    except AssertionError as e:
        print_test_result("FAIL", f"Test failed: {e}")
        record_test_result(test_name, "FAIL", str(e))
        log_test_end(test_name, "FAIL")
        raise


def test_analyze_starts_and_returns_status(app_and_client):
    test_name = "test_analyze_starts_and_returns_status"
    log_test_start(test_name)
    print_test_header(test_name)
    
    app_module, client = app_and_client
    
    print_test_step("Sending POST request to /analyze with valid UID and sessionId")
    resp = client.post("/analyze", json={"uid": "user123", "sessionId": "sessA"})
    
    print_response_data("Response Status", resp.status_code)
    data = resp.get_json()
    print_response_data("Response Data", data)
    
    log_test_result(test_name, f"Response status: {resp.status_code}")
    log_test_result(test_name, f"Response data: {data}")
    
    try:
        assert resp.status_code == 200
        assert data["status"] in ("running", "paused")
        assert data["studentId"] == "user123"
        assert "latest" in data
        # Uncomment the line below to force test failure
        # assert data["studentId"] == "wrong_user"  # This will fail - expecting user123
        print_test_result("PASS", "Analyze endpoint working correctly")
        record_test_result(test_name, "PASS", f"Analyzer started with status: {data['status']}")
        log_test_result(test_name, "TEST PASSED: Analyze endpoint working correctly")
        log_test_end(test_name, "PASS")
    except AssertionError as e:
        print_test_result("FAIL", f"Test failed: {e}")
        record_test_result(test_name, "FAIL", str(e))
        log_test_end(test_name, "FAIL")
        raise


def test_overall_no_docs_returns_empty(app_and_client):
    test_name = "test_overall_no_docs_returns_empty"
    log_test_start(test_name)
    print_test_header(test_name)
    
    app_module, client = app_and_client
    
    print_test_step("Initializing analyzer with UID")
    client.post("/analyze", json={"uid": "u1"})
    
    print_test_step("Sending GET request to /overall/u1")
    resp = client.get("/overall/u1")
    
    print_response_data("Response Status", resp.status_code)
    result = resp.get_json()
    print_response_data("Overall Data", result)
    
    log_test_result(test_name, f"Response status: {resp.status_code}")
    log_test_result(test_name, f"Overall data for u1: {result}")
    
    try:
        assert resp.status_code == 200
        assert result == []
        # Uncomment the line below to force test failure
        # assert len(result) > 0  # This will fail - expecting empty array
        print_test_result("PASS", "Empty overall data returned correctly")
        record_test_result(test_name, "PASS", "Empty array returned for no data")
        log_test_result(test_name, "TEST PASSED: Empty overall data returned correctly")
        log_test_end(test_name, "PASS")
    except AssertionError as e:
        print_test_result("FAIL", f"Test failed: {e}")
        record_test_result(test_name, "FAIL", str(e))
        log_test_end(test_name, "FAIL")
        raise


def test_overall_returns_latest_value(app_and_client, stub_modules):
    test_name = "test_overall_returns_latest_value"
    log_test_start(test_name)
    print_test_header(test_name)
    
    app_module, client = app_and_client
    db: FakeDB = stub_modules["db"]

    print_test_step("Creating session document in mock Firestore")
    sess_ref = db.collection(app_module.COL_SESSIONS).document("u2")
    sess_ref.set({"studentId": "u2", "sessionId": "S1"}, merge=True)

    print_test_step("Adding sample data with different timestamps")
    now = datetime.utcnow()
    sample1 = {
        "studentId": "u2",
        "sessionId": "S1",
        "overall": 71.678,
        "savedAt": now.timestamp(),
    }
    sample2 = {
        "studentId": "u2",
        "sessionId": "S1",
        "overall": 65.4,
        "savedAt": (now - timedelta(seconds=10)).timestamp(),
    }
    
    print_response_data("Sample 1 (Latest)", sample1)
    print_response_data("Sample 2 (Older)", sample2)
    
    log_test_result(test_name, f"Adding sample1: {sample1}")
    log_test_result(test_name, f"Adding sample2: {sample2}")
    
    sess_ref.collection(app_module.COL_SAMPLES).add(sample1)
    sess_ref.collection(app_module.COL_SAMPLES).add(sample2)

    print_test_step("Sending GET request to /overall/u2")
    resp = client.get("/overall/u2")
    
    print_response_data("Response Status", resp.status_code)
    data = resp.get_json()
    print_response_data("Overall Data", data)
    
    log_test_result(test_name, f"Response status: {resp.status_code}")
    log_test_result(test_name, f"Overall data for u2: {data}")
    
    try:
        assert resp.status_code == 200
        assert isinstance(data, list) and len(data) == 1
        assert data[0]["overall"] == 71.68
        # Uncomment the line below to force test failure
        # assert data[0]["overall"] == 65.4  # This will fail - expecting latest value 71.68
        print_test_result("PASS", "Latest overall value returned correctly (71.68)")
        record_test_result(test_name, "PASS", f"Latest value: {data[0]['overall']}")
        log_test_result(test_name, "TEST PASSED: Latest overall value returned correctly")
        log_test_end(test_name, "PASS")
    except AssertionError as e:
        print_test_result("FAIL", f"Test failed: {e}")
        record_test_result(test_name, "FAIL", str(e))
        log_test_end(test_name, "FAIL")
        raise


def test_all_analysis_requires_init(app_and_client):
    test_name = "test_all_analysis_requires_init"
    log_test_start(test_name)
    print_test_header(test_name)
    
    app_module, client = app_and_client
    
    print_test_step("Sending GET request to /all_analysis")
    resp = client.get("/all_analysis")
    
    print_response_data("Response Status", resp.status_code)
    response_data = resp.get_json()
    print_response_data("Response Data", response_data)
    
    log_test_result(test_name, f"Initial all_analysis response status: {resp.status_code}")
    log_test_result(test_name, f"Initial all_analysis response: {response_data}")
    
    try:
        if resp.status_code == 400:
            print_test_result("INFO", "Analyzer not initialized - testing initialization flow")
            assert "Analyzer not initialized" in response_data["message"]
            
            print_test_step("Initializing analyzer with /analyze endpoint")
            resp2 = client.post("/analyze", json={"uid": "u3"})
            print_response_data("Analyze Response Status", resp2.status_code)
            print_response_data("Analyze Response", resp2.get_json())
            
            log_test_result(test_name, f"Analyze response status: {resp2.status_code}")
            log_test_result(test_name, f"Analyze response: {resp2.get_json()}")
            assert resp2.status_code == 200

            print_test_step("Testing /all_analysis after initialization")
            resp3 = client.get("/all_analysis")
            print_response_data("Final Response Status", resp3.status_code)
            payload = resp3.get_json()
            print_response_data("Final Response Data", payload)
            
            log_test_result(test_name, f"Final all_analysis response status: {resp3.status_code}")
            log_test_result(test_name, f"Final all_analysis response: {payload}")
            assert resp3.status_code == 200
            assert payload["studentId"] == "u3"
            assert "data" in payload
            assert isinstance(payload["data"], dict)
            
            print_test_result("PASS", "All analysis initialization working correctly")
            record_test_result(test_name, "PASS", "Initialization flow works correctly")
            log_test_result(test_name, "TEST PASSED: All analysis initialization working correctly")
            log_test_end(test_name, "PASS")
        else:
            print_test_result("INFO", "Analyzer already initialized - testing with existing analyzer")
            assert resp.status_code == 200
            payload = response_data
            assert "data" in payload
            assert isinstance(payload["data"], dict)
            
            print_test_step("Testing UID update with existing analyzer")
            resp2 = client.post("/analyze", json={"uid": "u3"})
            print_response_data("Analyze Response Status", resp2.status_code)
            print_response_data("Analyze Response", resp2.get_json())
            
            log_test_result(test_name, f"Analyze response status: {resp2.status_code}")
            log_test_result(test_name, f"Analyze response: {resp2.get_json()}")
            assert resp2.status_code == 200
            
            print_test_step("Verifying updated studentId")
            resp3 = client.get("/all_analysis")
            print_response_data("Updated Response Status", resp3.status_code)
            payload3 = resp3.get_json()
            print_response_data("Updated Response Data", payload3)
            
            log_test_result(test_name, f"Updated all_analysis response status: {resp3.status_code}")
            log_test_result(test_name, f"Updated all_analysis response: {payload3}")
            assert resp3.status_code == 200
            assert payload3["studentId"] == "u3"
            # Uncomment the line below to force test failure
            # assert payload3["studentId"] == "wrong_user"  # This will fail - expecting u3
            
            print_test_result("PASS", "All analysis working with pre-initialized analyzer")
            record_test_result(test_name, "PASS", "Pre-initialized analyzer works correctly")
            log_test_result(test_name, "TEST PASSED: All analysis working with pre-initialized analyzer")
            log_test_end(test_name, "PASS")
    except AssertionError as e:
        print_test_result("FAIL", f"Test failed: {e}")
        record_test_result(test_name, "FAIL", str(e))
        log_test_end(test_name, "FAIL")
        raise


def test_analyzer_start_stop_functionality(app_and_client):
    test_name = "test_analyzer_start_stop_functionality"
    log_test_start(test_name)
    print_test_header(test_name)
    
    app_module, client = app_and_client
    
    print_test_step("Starting analyzer with /analyze endpoint")
    resp = client.post("/analyze", json={"uid": "u4", "sessionId": "sessB"})
    
    print_response_data("Analyze Response Status", resp.status_code)
    data = resp.get_json()
    print_response_data("Analyze Response Data", data)
    
    log_test_result(test_name, f"Analyze response status: {resp.status_code}")
    log_test_result(test_name, f"Analyze response: {data}")
    
    try:
        assert resp.status_code == 200
        assert data["status"] == "running"
        assert data["studentId"] == "u4"
        # Uncomment the line below to force test failure
        # assert data["status"] == "stopped"  # This will fail - expecting running status
        
        print_test_step("Testing /pause endpoint")
        resp2 = client.post("/pause")
        
        print_response_data("Pause Response Status", resp2.status_code)
        pause_data = resp2.get_json()
        print_response_data("Pause Response Data", pause_data)
        
        log_test_result(test_name, f"Pause response status: {resp2.status_code}")
        log_test_result(test_name, f"Pause response: {pause_data}")
        
        if resp2.status_code == 404:
            print_test_result("WARNING", "/pause endpoint not implemented")
            print_test_step("Checking analyzer status directly")
            
            log_test_result(test_name, "NOTICE: /pause endpoint not implemented")
            log_test_result(test_name, "Checking available endpoints...")
            
            if hasattr(app_module, 'analyzer') and app_module.analyzer:
                print_test_result("INFO", f"Analyzer tracking status: {app_module.analyzer.is_tracking}")
                log_test_result(test_name, f"Analyzer tracking status: {app_module.analyzer.is_tracking}")
                print_test_result("PASS", "Analyzer initialized and accessible")
                record_test_result(test_name, "PASS", "Analyzer running, control endpoints not implemented")
                log_test_result(test_name, "TEST PASSED: Analyzer initialized and running")
                log_test_end(test_name, "PASS")
            else:
                print_test_result("PASS", "Analyze endpoint working, control endpoints not implemented")
                record_test_result(test_name, "PASS", "Basic functionality works, advanced controls missing")
                log_test_result(test_name, "TEST PASSED: Analyze endpoint working, pause/resume/stop endpoints not implemented")
                log_test_end(test_name, "PASS")
        else:
            print_test_result("INFO", "Control endpoints available - testing full flow")
            assert resp2.status_code == 200
            assert pause_data["status"] == "paused"
            
            print_test_step("Testing /resume endpoint")
            resp3 = client.post("/resume")
            print_response_data("Resume Response Status", resp3.status_code)
            resume_data = resp3.get_json()
            print_response_data("Resume Response Data", resume_data)
            
            log_test_result(test_name, f"Resume response status: {resp3.status_code}")
            log_test_result(test_name, f"Resume response: {resume_data}")
            assert resp3.status_code == 200
            assert resume_data["status"] == "running"
            
            print_test_step("Testing /stop endpoint")
            resp4 = client.post("/stop")
            print_response_data("Stop Response Status", resp4.status_code)
            stop_data = resp4.get_json()
            print_response_data("Stop Response Data", stop_data)
            
            log_test_result(test_name, f"Stop response status: {resp4.status_code}")
            log_test_result(test_name, f"Stop response: {stop_data}")
            assert resp4.status_code == 200
            assert stop_data["status"] == "stopped"
            
            print_test_result("PASS", "Full analyzer control functionality working correctly")
            record_test_result(test_name, "PASS", "All control endpoints working")
            log_test_result(test_name, "TEST PASSED: Analyzer start/stop functionality working correctly")
            log_test_end(test_name, "PASS")
    except AssertionError as e:
        print_test_result("FAIL", f"Test failed: {e}")
        record_test_result(test_name, "FAIL", str(e))
        log_test_end(test_name, "FAIL")
        raise
